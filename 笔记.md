# 人工智能课程核心

## 产业链

### 上游/基础层

- 传感器：红外、温湿度、指纹、图像（韦尔股份、奥比中光..）
- 数据采集的知识产权问题：数据确权
- 大脑
  - 云计算平台（云存储、云计算、综合云）
  - 算力：基础算力（cpu）、智能算力（gpu）、超算算力
- 硬件：EDA、存储芯片.. 立芯科技

### 中游/技术层

- 大模型 LLM：华为盘古、阿里通义、腾讯混元、百度文心...（通用大模型） 夸克浏览器（医疗大模型）
- 知识图谱/思维导图
- 操作系统、深度学习框架（pytorch、tensorflow）（国产：华为 mindspore、百度 paddle..）
- 应用技术
  - 计算机视觉 cv
  - 语音识别 科大讯飞
  - 自然语言处理 chatgpt、gpt4.0
  - 机器视觉：模拟人的眼睛的作用

### 下游/应用层

- 应用场景
  - 游戏：王者觉悟
  - 无人物流、无人零售、医疗、智慧城市（杭州）

## 学习路径

- 学 python（基本语法、面向对象）
- 数据处理分析（numpy、pandas、matplotlib）
- 机器学习概念（线性回归、逻辑回归、决策树、随机森林、SVM...）
- 深度学习
  - pytorch、anaconda
  - 零基础项目：手写数字识别、波士顿房价预测
  - 实际项目

## 课程安排

- 参考书：清华大学出版 《人工智能》 马少平
- 18 小时 12 次课
- 第 0 章绪论+第 1 章搜索第 1 节
- 第 1 章搜索 2-5 节
- 第 2 章与或图
- 第 3 章谓词逻辑
- 第 4 章知识表示
- 第 5 章不确定性推理方法 1-4 节
- 第 5 章不确定性推理方法 5-6 节+第 6 章机器学习基本概念
- 第 6 章机器学习
- 第 6 章机器学习
- 第 7 章高级搜索

## 考试形式预测

- 大题（计算）

  - A\*算法
  - 极大极小搜索+α-β 剪枝
  - 谓词逻辑推理（离散数学）
  - 语义网络
  - D 分离的判断+CF 的传播、更新
  - ID3 算法计算
  - 最后补充：主观贝叶斯
- 小题（常识+概念）

  - 简答
  - 选择题
  - 判断题
  - 填空题

## 人工智能基本概念

图灵测试

中文屋子

近年来的重要时间结点

- 2016 年 deepmind 击败李世石
- 2017 年 《attention is all your need》
- 2018 年 google 提出的 bert 模型
- 2022 年 openai 提出的 chatgpt
- 2024 年 openai 提出的 sora（文本生成视频的大模型）

三大学派

- **符号主义**：立足于逻辑运算和符号操作,适合于模拟人的逻辑思维过程,解决需要逻辑推理的复杂问题。符号主义认为人类思维的基本单元是符号，而基于符号的一系列运算就构成了认知的过程，所以人和计算机都可以被看成具备逻辑推理能力的符号系统，换句话说，计算机可以通过各种符号运算来模拟人的“智能”。应用：**专家系统**
- **连接主义**：通过过**_神经元_**之间的并行协作实现信息处理,处理过程具有并行性,动态性,全局性。也被业界称为**仿生学派**，其主要原理为**神经网络**及神经网络间的连接机制与学习算法。认为人工智能源于仿生学,特别是对人脑模型的研究。它的代表性成果是 1943 年由生理学家麦卡洛克(McCulloch)和数理逻辑学家皮茨(Pitts)创立的脑模型,即 MP 模型,开创了用电子装置模仿人脑结构和功能的新途径。它从神经元开始进而研究神经网络模型和脑模型,开辟了人工智能的又一发展道路。20 世纪 60~70 年代,连接主义,尤其是对以感知机(perceptron)为代表的脑模型的研究出现过热潮,由于受到当时的理论模型、生物原型和技术条件的限制,脑模型研究在 20 世纪 70 年代后期至 80 年代初期落入低潮。直到 Hopfield 教授在 1982 年和 1984 年发表两篇重要论文,提出用硬件模拟神经网络以后,连接主义才又重新抬头。1986 年,鲁梅尔哈特(Rumelhart)等人提出多层网络中的反向传播(BP)算法。
- **行为主义**：采用行为**模拟方法**，也认为功能、结构和智能行为是不可分的。不同行为表现出不同功能和不同控制结构。行为主义学派认为人工智能源于**控制\*\***论\*\*。早期的研究工作重点是模拟人在控制过程中的智能行为和作用，并进行“控制论动物”的研制。到 20 世纪 60~70 年代，播下智能控制和智能机器人的种子，并在 20 世纪 80 年代诞生了智能控制和智能机器人系统。行为主义是 20 世纪末才以人工智能新学派的面孔出现的，引起了许多人的兴趣。这一学派的代表作首推布鲁克斯的六足行走机器人，它被看作是新一代的“控制论动物”，是一个基于感知-动作模式模拟昆虫行为的控制系统。

发展历史

**神经网络**

在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。

**神经元模型**

神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示。

![1.png](https://i.loli.net/2018/10/17/5bc72cbb6cc11.png)

一直沿用至今的“M-P 神经元模型”正是对这一结构进行了抽象，也称“**阈值逻辑单元**“，其中树突对应于输入部分，每个神经元收到 n 个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和 Σxi\*wi，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。**激活函数负责将神经元的输入映射到输出端，给神经元引入了非线性因素**，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。M-P 神经元模型如下图所示：

![2.png](https://i.loli.net/2018/10/17/5bc72cbb7be44.png)

与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值 1 或 0，若差值大于零输出 1，对应兴奋；若差值小于零则输出 0，对应抑制。但阶跃函数不连续，不光滑，故在 M-P 神经元模型中，也采用 Sigmoid 函数来近似， Sigmoid 函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。

**常用的激活函数及其导数：白皮书 p305 6.**

![3.png](https://i.loli.net/2018/10/17/5bc72cbb40dc5.png)

将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说 10 个神经元两两连接，则有 100 个参数需要学习（每个神经元有 9 个连接权以及 1 个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。

**感知机与多层网络**

感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是 M-P 神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型（最小二乘法）中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用 sigmoid 函数将这个输出值压缩到 0-1 之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。

给定训练集，则感知机的 n+1 个参数（n 个权重+1 个阈值）都可以通过学习得到。阈值 Θ 可以看作一个输入值固定为-1 的哑结点的权重 ωn+1，即假设有一个固定输入 xn+1=-1 的输入层神经元，其对应的权重为 ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示：

![4.png](https://i.loli.net/2018/10/17/5bc72cbb3fdf0.png)

感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：

![5.png](https://i.loli.net/2018/10/17/5bc72cbb3ba63.png)

其中 **η∈（0，1）称为学习率**，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。

由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的**异或问题**，只通过一层功能神经元往往不能解决，因此要解决**非线性可分问题**，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：

![6.png](https://i.loli.net/2018/10/17/5bc72cbb58ec6.png)

在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“**多层前馈神经网络**”（multi-layer feedforward neural network），该结构满足以下几个特点：

- 每层神经元与下一层神经元之间完全互连
- 神经元之间不存在同层连接
- 神经元之间不存在跨层连接

![7.png](https://i.loli.net/2018/10/17/5bc72cbb47ff8.png)

根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中**不存在环或回路**，而不是指该网络只能向前传播而不能向后传播（下节中的 BP 神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。

**BP 神经网络算法**

由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP 神经网络算法即**误差逆传播算法**（error BackPropagation）正是为学习多层前馈神经网络而设计，BP 神经网络算法是迄今为止最成功的的神经网络学习算法。

一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数。

![8.png](https://i.loli.net/2018/10/17/5bc72cbb92ff5.png)

上图为一个单隐层前馈神经网络的拓扑结构，BP 神经网络算法也使用**梯度下降法**（gradient descent）。BP 算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的**均方误差 MSE**，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。
![12.png](https://pic2.zhimg.com/80/v2-8561cbd890dafcf5f173394f2d79c5b1_720w.webp)

学习率 η∈（0，1）控制着沿反梯度方向下降的步长，若**步长太大则下降太快容易产生震荡**，若步长太小则收敛速度太慢，一般地常把 η 设置为 0.1（基于经验设置），有时更新权重时会将输出层与隐含层设置为不同的学习率。BP 算法的基本流程如下所示：

![10.png](https://i.loli.net/2018/10/17/5bc72cbb59e99.png)

BP 算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即 BP 算法每次更新只针对于单个样例。需要注意的是：BP 算法的最终目标是要最小化整个训练集 D 上的累积误差，即：

![11.png](https://i.loli.net/2018/10/17/5bc72ce222a96.png)

如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准 BP 算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准 BP 算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。

**大模型幻觉（hallucination）：一本正经的胡说八道 / 可能是大模型重要的创造力来源**

过拟合（overfitting）

- 表现：训练集表现好、测试集表现差
- 原因：模型过于复杂、学习到了数据中不是通用性的特征、记住了数据中的噪声和细节

BP 神经网络强大的学习能力常常容易造成**过拟合**问题，有以下两种策略来**缓解** BP 网络的**过拟合问题**：

- 早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。
- 引入正则化（regularization）：基本思想是在**累积误差函数**中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中 λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计，即降低模型复杂度
- 数据预处理，对数据进行标准化、归一化
- 增加数据集
- **集成学习：将多个不一样模型通过投票（离散值）/平均（连续值）的方式得到最终预测结果，降低模型的方差，提高泛化性能**
  ![13.png](https://i.loli.net/2018/10/17/5bc72ce227ff1.png)

## 搜索

- 回溯（2 道经典问题 : n 皇后 和 求子集）

  - n 皇后：将 n 个皇后放在 n×n 的棋盘中，使得皇后之间不能互相攻击（不能同行、不能同列、不能同对角线），求 n 皇后的解决方案数量有几种？
  - https://leetcode.cn/problems/n-queens/
  - 会画搜索树
  - 会写回溯的代码（预防上机考试 保证有 c 和 c++，可能有 java 和 python）
    ![4皇后](./imgs/1.png)
  - eg1.给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（手写代码？）
  - https://leetcode.cn/problems/subsets/
  - github (学会使用梯子、git 命令基本操作)

  ```JavaScript
  输入：nums = [1,2,3]
  //注意空子集
  输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]]
  ```

  ```JavaScript
  // 回溯模板
  // void backtracking(参数)
  // {
  //     if (终止条件)
  //     {
  //         存放结果;
  //         return;
  //     }
  //     for(选择本层的所有元素/树中节点的子节点的数量){
  //         处理节点;
  //         backtacking();
  //         撤销处理;
  //     }
  // }

  var subsets = function (nums) {
      const result = []

      function backTrack(start, curr) {
          // 把curr加入result数组
          result.push([...curr])
          for (let i = start; i < nums.length; i++) {
              curr.push(nums[i])
              backTrack(i + 1, curr)
              // 把curr最后一个元素移除
              curr.pop()
          }
      }

      backTrack(0, [])
      return result
  }
  ```
- 图搜索

  - 深度优先
  - 广度优先
  - 启发式搜索 A\* （A star）**只考填表，不会考代码**

    - 1968 年提出的基于采样搜索的路径规划算法
    - 处理例子：

      - 网格化：连续问题转化为离散
      - 每次都取局部最优，未必能有全局最优
      - 处理策略 1：每次都取离终点最近的那个，但是贪心策略本身有问题
      - 处理策略 2：选取离起点（历史数据）和终点的距离之和最小
      - 为什么考虑历史数据就能避免贪心带来的问题？
      - 把距离称为代价f，和起点的距离称为历史代价g和终点的距离称为未来预期代价h
    - **f=g+h**
    - g 表示从起点到当前位置的距离
    - h 表示从当前位置到终点的估算距离

      - 曼哈顿距离 用于只能朝上下左右 4 个方向移动 d=|x1-x2|+|y1-y2|，基本只在二维平面使用
      - 欧几里得距离 任意方向移动 多维可用
    - 应用举例 迷宫路径搜索 地图最佳路径规划
  - 动态规划法（上机必考、面试问答）

    - 基本概念
      - 动态规划算法通常用于求解具有某种最优性质的问题。在这类问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解
      - 动态规划算法与分治法类似，其基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次
      - 如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中
    - eg1.假设你正在爬楼梯。需要 n 阶你才能到达楼顶。**每次你可以爬 1 或 2 个台阶**。你有多少种不同的方法可以爬到楼顶呢？
    - 用f（x）表示爬到第x级台阶的方案数量 ==》**f（x）=f（x-1）+f（x-2）**
    - 从第0级爬到第0级：f（0）=1
    - 从第0级爬到第1级：f（1）=1
    - 爬到第2级：f（2）=f（0）+f（1）=2，以此类推...

    ```JavaScript
    var climbStairs = function (n) {
        const memo = []
        // 到第1个台阶有1种方法
        memo[1] = 1
        // 到第2个台阶有2种方法
        memo[2] = 2
        for (let i = 3; i <= n; i++) {
            memo[i] = memo[i - 1] + memo[i - 2]
        }
        return memo[n]
    }
    ```

    - eg2.你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，**如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警**。给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。
    - 确定数组含义：dp【i】表示在第i号房子**之前**能偷到的最大金额
    - 状态转移：
      - 第i-2号房子前可偷窃的金额+当前i号房子可偷窃金额
      - 第i-1号房子前可偷窃的金额
      - 二者取最大值作为当前的dp【i】
      - 方程：dp【i】=max（dp【i-2】+nums【i】，dp【i-1】）
    - dp数组初始化
      - dp【0】= nums【0】
      - dp【1】= max（nums【0】，nums【1】）

    ```JavaScript
    输入：[2,7,9,3,1]
    输出：12
    解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。偷窃到的最高金额 = 2 + 9 + 1 = 12
    ```

    ```JavaScript
    var rob = function (nums) {
        if (nums.length === 0) {
            return 0
        }
        if (nums.length === 1) {
            return nums[0]
        }

        // 记录每个点能偷到的最大金额
        const memo = []
        memo[0] = nums[0]
        memo[1] = Math.max(nums[0], nums[1])
        for (let i = 2; i < nums.length; i++) {
            // 比较 偷当前+前2个能偷的最大值 和 前1个能偷的最大值
            memo[i] = Math.max(nums[i] + memo[i - 2], memo[i - 1])
        }

        return memo[nums.length - 1]
    }
    ```